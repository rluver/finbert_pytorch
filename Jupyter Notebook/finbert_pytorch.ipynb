{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertConfig, BertTokenizer, BertForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorizer(label):\n",
    "    \n",
    "    if label == 'positive':\n",
    "        return 2\n",
    "    elif label == 'neutral':\n",
    "        return 1\n",
    "    elif label == 'negative':\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "def convert_idx_to_sentiment(idx):\n",
    "    \n",
    "    return ['negative', 'neutral', 'positive'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialPhraseBankDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            data: pd.DataFrame,\n",
    "            tokenizer: BertTokenizer,\n",
    "            text_max_token_length: int = 512\n",
    "            ):\n",
    "                \n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.text_max_token_length = text_max_token_length\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        data_row = self.data.iloc[index]\n",
    "    \n",
    "        encoded_text = self.tokenizer.encode_plus(\n",
    "            data_row.phrase,\n",
    "            max_length = self.text_max_token_length, \n",
    "            padding = 'max_length',\n",
    "            truncation = True, \n",
    "            return_attention_mask = True, \n",
    "            add_special_tokens = True, \n",
    "            return_tensors = 'pt'\n",
    "            )\n",
    "        \n",
    "        return dict(\n",
    "            input_ids = encoded_text.input_ids.flatten(),\n",
    "            attention_mask = encoded_text.attention_mask.flatten(),\n",
    "            token_type_ids = encoded_text.token_type_ids.flatten(),\n",
    "            label = torch.tensor(data_row.sentiment).unsqueeze(0)\n",
    "            )\n",
    "    \n",
    "    \n",
    "    \n",
    "class FinancialPhraseBankDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(            \n",
    "        self,\n",
    "        train_df: pd.DataFrame,\n",
    "        test_df: pd.DataFrame,\n",
    "        tokenizer: BertTokenizer,\n",
    "        batch_size: int = 64,\n",
    "        text_max_token_length: int = 512,\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_max_token_length = text_max_token_length\n",
    "        \n",
    "        self.setup()\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_df)\n",
    "        \n",
    "\n",
    "\n",
    "    def setup(self, stage = None):\n",
    "        self.train_dataset = FinancialPhraseBankDataset(\n",
    "            self.train_df,\n",
    "            self.tokenizer,\n",
    "            self.text_max_token_length,\n",
    "            )\n",
    "        \n",
    "        self.test_dataset = FinancialPhraseBankDataset(\n",
    "            self.test_df,\n",
    "            self.tokenizer,\n",
    "            self.text_max_token_length,\n",
    "            )\n",
    "    \n",
    "    \n",
    "    def train_dataloader(self):        \n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size = self.batch_size,\n",
    "            shuffle = False\n",
    "            )\n",
    "\n",
    "    \n",
    "    def val_dataloader(self):        \n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size = self.batch_size,\n",
    "            shuffle = False\n",
    "            )\n",
    "    \n",
    "    \n",
    "    def test_dataloader(self):        \n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size = self.batch_size,\n",
    "            shuffle = False\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinBERT(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, model_path = 'model', train_samples = 3388, batch_size = 64, epochs = 10, num_labels = 3, learning_rate = 2e-5, discriminative_fine_tuning_rate = 0.85):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.learning_rate = learning_rate\n",
    "        self.discriminative_fine_tuning_rate = discriminative_fine_tuning_rate\n",
    "        self.train_samples = train_samples\n",
    "        self.batch_size = batch_size\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.epochs = epochs\n",
    "        self.warm_up_proportion = 0.2\n",
    "        self.num_train_optimization_steps = int(self.train_samples / self.batch_size / self.gradient_accumulation_steps) * epochs\n",
    "        self.num_warmup_steps = int(float(self.num_train_optimization_steps) * self.warm_up_proportion)\n",
    "\n",
    "\n",
    "        self.no_decay_layer_list = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "\n",
    "        config = BertConfig.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
    "        config.num_labels = num_labels\n",
    "        self.bert_model = BertForSequenceClassification.from_pretrained(model_path, config = config)\n",
    "        \n",
    "        self.optimizer_grouped_parameters = self.get_optimizer_grouped_parameters()\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels = None):        \n",
    "        output = self.bert_model(\n",
    "            input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids = token_type_ids,\n",
    "            labels = labels\n",
    "            )\n",
    "         \n",
    "        return output.loss, output.logits\n",
    "    \n",
    "    \n",
    "    def get_optimizer_grouped_parameters(self):\n",
    "        \n",
    "        discriminative_fine_tuning_encoders = []\n",
    "        for i in range(12):\n",
    "            ith_layer = list(self.bert_model.bert.encoder.layer[i].named_parameters())\n",
    "            \n",
    "            encoder_decay = {\n",
    "                'params': [param for name, param in ith_layer if\n",
    "                           not any(no_decay_layer_name in name for no_decay_layer_name in self.no_decay_layer_list)],\n",
    "                'weight_decay': 0.01,\n",
    "                'lr': self.learning_rate / (self.discriminative_fine_tuning_rate ** (12 - i))\n",
    "                }\n",
    "        \n",
    "            encoder_nodecay = {\n",
    "                'params': [param for name, param in ith_layer if\n",
    "                           any(no_decay_layer_name in name for no_decay_layer_name in self.no_decay_layer_list)],\n",
    "                'weight_decay': 0.0,\n",
    "                'lr': self.learning_rate / (self.discriminative_fine_tuning_rate ** (12 - i))}\n",
    "            \n",
    "            discriminative_fine_tuning_encoders.append(encoder_decay)\n",
    "            discriminative_fine_tuning_encoders.append(encoder_nodecay)\n",
    "            \n",
    "        \n",
    "        embedding_layer = self.bert_model.bert.embeddings.named_parameters()\n",
    "        pooler_layer = self.bert_model.bert.pooler.named_parameters()\n",
    "        classifier_layer = self.bert_model.classifier.named_parameters()\n",
    "        \n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [param for name, param in embedding_layer if\n",
    "                        not any(no_decay_layer_name in name for no_decay_layer_name in self.no_decay_layer_list)],\n",
    "             'weight_decay': 0.01,\n",
    "             'lr': self.learning_rate / (self.discriminative_fine_tuning_rate ** 13)},\n",
    "            {'params': [param for name, param in embedding_layer if\n",
    "                        any(no_decay_layer_name in name for no_decay_layer_name in self.no_decay_layer_list)],\n",
    "             'weight_decay': 0.0,\n",
    "             'lr': self.learning_rate / (self.discriminative_fine_tuning_rate ** 13)},\n",
    "            {'params': [param for name, param in pooler_layer if\n",
    "                        not any(no_decay_layer_name in name for no_decay_layer_name in self.no_decay_layer_list)],\n",
    "             'weight_decay': 0.01,\n",
    "             'lr': self.learning_rate},\n",
    "            {'params': [param for name, param in pooler_layer if\n",
    "                        any(no_decay_layer_name in name for no_decay_layer_name in self.no_decay_layer_list)],\n",
    "             'weight_decay': 0.0,\n",
    "             'lr': self.learning_rate},\n",
    "            {'params': [param for name, param in classifier_layer if\n",
    "                        not any(no_decay_layer_name in name for no_decay_layer_name in self.no_decay_layer_list)],\n",
    "             'weight_decay': 0.01,\n",
    "             'lr': self.learning_rate},\n",
    "            {'params': [param for name, param in classifier_layer if\n",
    "                        any(no_decay_layer_name in name for no_decay_layer_name in self.no_decay_layer_list)],\n",
    "             'weight_decay': 0.0,\n",
    "             'lr': self.learning_rate}            \n",
    "            ]\n",
    "                \n",
    "        optimizer_grouped_parameters.extend(discriminative_fine_tuning_encoders)\n",
    "        \n",
    "        return optimizer_grouped_parameters\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_index):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        token_type_ids = batch['token_type_ids']\n",
    "        label = batch['label']\n",
    "        \n",
    "        loss, logits = self(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids = token_type_ids,\n",
    "            labels = label\n",
    "            )\n",
    "        \n",
    "        total = label.size(0)        \n",
    "        pred = torch.argmax(logits, 1).unsqueeze(1)\n",
    "        correct = (pred == label).sum().item()\n",
    "        acc = correct/total\n",
    "\n",
    "        \n",
    "        self.log('train_loss', loss, prog_bar = True, logger = True)\n",
    "        self.log('train_acc', acc, prog_bar = True, logger = True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_index):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        token_type_ids = batch['token_type_ids']\n",
    "        label = batch['label']\n",
    "\n",
    "        \n",
    "        loss, logits = self(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids = token_type_ids,\n",
    "            labels = label\n",
    "            )\n",
    "        \n",
    "        total = label.size(0)        \n",
    "        pred = torch.argmax(logits, 1).unsqueeze(1)\n",
    "        correct = (pred == label).sum().item()\n",
    "        acc = correct/total\n",
    "\n",
    "        self.log('val_acc', acc, prog_bar = True, logger = True)\n",
    "        self.log('val_loss', loss, prog_bar = True, logger = True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def test_step(self, batch, batch_index):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        token_type_ids = batch['token_type_ids']        \n",
    "        label = batch['label']\n",
    "\n",
    "        loss, logits = self(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids = token_type_ids,\n",
    "            labels = label\n",
    "            )\n",
    "        \n",
    "        total = label.size(0)        \n",
    "        pred = torch.argmax(logits, 1).unsqueeze(1)\n",
    "        correct = (pred == label).sum().item()\n",
    "        acc = correct/total\n",
    "        \n",
    "        self.log('test_acc', acc, prog_bar = True, logger = True)\n",
    "        self.log('test_loss', loss, prog_bar = True, logger = True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        optimizer = AdamW(\n",
    "            self.optimizer_grouped_parameters,\n",
    "            lr = self.learning_rate,\n",
    "            correct_bias = False\n",
    "            )\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps = self.num_warmup_steps,\n",
    "            num_training_steps = self.num_train_optimization_steps\n",
    "            )\n",
    "        \n",
    "        return [optimizer], [{'scheduler': scheduler, 'interval': 'step'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_phrase_dataset = pd.read_csv('dataset/financial_phrase_bank/all-data.csv', encoding = 'latin-1', names = ['sentiment', 'phrase']).drop_duplicates().dropna().reset_index(drop = True)\n",
    "financial_phrase_dataset.sentiment = financial_phrase_dataset.sentiment.apply(lambda x: categorizer(x))\n",
    "train, test = train_test_split(financial_phrase_dataset, test_size = 0.3, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at model were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "NUM_LABELS = 3\n",
    "LEARNING_RATE = 2e-5    \n",
    "DISCRIMINATIVE_FINE_TUNING_RATE = 0.85\n",
    "\n",
    "data_module = FinancialPhraseBankDataModule(train, test, tokenizer, batch_size = BATCH_SIZE)    \n",
    "model = FinBERT(model_path = 'model', train_samples = len(data_module), batch_size = BATCH_SIZE, epochs = EPOCHS, num_labels = NUM_LABELS, learning_rate = LEARNING_RATE, discriminative_fine_tuning_rate = DISCRIMINATIVE_FINE_TUNING_RATE)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath = 'checkpoints',\n",
    "    filename = 'best-checkpoint',\n",
    "    save_top_k = 1,\n",
    "    verbose = True,\n",
    "    monitor = 'val_loss',\n",
    "    mode = 'min'\n",
    "    )\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    patience = 2,\n",
    "    mode = 'min'\n",
    "    )\n",
    "\n",
    "logger = TensorBoardLogger('lightning_logs', name = 'finbert_sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type                          | Params\n",
      "-------------------------------------------------------------\n",
      "0 | bert_model | BertForSequenceClassification | 109 M \n",
      "1 | criterion  | CrossEntropyLoss              | 0     \n",
      "-------------------------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "437.938   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02fdd844e7fd49d3a27fe389f417e1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 105: val_loss reached 0.49629 (best 0.49629), saving model to \"/home/ubuntu/Documents/shared/python_code/finbert/checkpoints/best-checkpoint.ckpt\" as top 1\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 211: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 317: val_loss was not in top 1\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    logger = logger,\n",
    "    callbacks = [checkpoint_callback, early_stopping],\n",
    "    max_epochs = EPOCHS,\n",
    "    gpus = 1,\n",
    "    progress_bar_refresh_rate = 1\n",
    "    )\n",
    "\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    \n",
    "    encoded_text = tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length = 512,\n",
    "        padding = 'max_length',\n",
    "        truncation = True,\n",
    "        return_attention_mask = True,\n",
    "        add_special_tokens = True,\n",
    "        return_tensors = 'pt'\n",
    "        )\n",
    "    \n",
    "    logit_output = sentiment_model(\n",
    "        input_ids = encoded_text.input_ids.flatten().unsqueeze(0),\n",
    "        attention_mask = encoded_text.attention_mask.flatten().unsqueeze(0),\n",
    "        token_type_ids = encoded_text.token_type_ids.flatten().unsqueeze(0)\n",
    "        )[-1]\n",
    "    \n",
    "    predicted_sentiment = torch.argmax(logit_output, 1)\n",
    "    \n",
    "    return convert_idx_to_sentiment(predicted_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at model were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sentiment_model = FinBERT.load_from_checkpoint('checkpoints/best-checkpoint.ckpt')\n",
    "sentiment_model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentiment('Apple, Facebook and Microsoft Are Still Undervalued')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentiment('Appleâ€™s new debt deal could mean more shareholder rewards after blowout earnings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentiment('Apple is delaying its office return by at least a month because of rising COVID-19 cases, report says')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentiment('''iPhone 13 release date: Apple's new flagship could be coming in September''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
